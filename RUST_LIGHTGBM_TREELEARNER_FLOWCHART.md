# Rust LightGBM TreeLearner Implementation Flowchart

## Overview
This document provides a comprehensive flowchart illustrating the implementation logic of the **TreeLearner** component in the Pure Rust LightGBM framework, specifically focusing on the tree learning algorithms located in `src/tree/`.

‚ö†Ô∏è **IMPORTANT**: This is a partial implementation compared to the C++ LightGBM. See [Missing Features](#missing-features) section for details on unimplemented functionality.

## Legend

### Flowchart Symbols
- **Rectangle** ‚¨ú: Process/Computation step
- **Diamond** ‚óá: Decision point/Conditional branch
- **Rounded Rectangle** ‚ö™: Start/End/Data structures
- **Parallelogram** ‚¨ü: Input/Output operations
- **Hexagon** ‚¨°: Mathematical computation

### Data Type Abbreviations
- `usize`: Platform-specific unsigned integer
- `DataSize`: Type alias for data size (typically `u32`)
- `FeatureIndex`: Type alias for feature index (typically `usize`) 
- `NodeIndex`: Type alias for node index (typically `usize`)
- `BinIndex`: Type alias for bin index (typically `u32`)
- `Score`: Type alias for prediction scores (typically `f32`)
- `Hist`: Type alias for histogram values (typically `f64`)
- `Array1<T>`: 1D ndarray of type T
- `Array2<T>`: 2D ndarray of type T
- `Vec<T>`: Dynamic vector of type T

## Core Data Structures

### SplitInfo Structure
‚ö†Ô∏è **NOTE**: Current implementation missing monotonic constraints and categorical support (Issues #102, #103)

```rust
pub struct SplitInfo {
    pub feature: FeatureIndex,              // Feature index for the split
    pub threshold_bin: BinIndex,            // Bin threshold for the split
    pub threshold_value: f64,               // Actual threshold value
    pub gain: f64,                          // Split gain (improvement in loss function)
    pub left_sum_gradient: f64,             // Left child gradient sum
    pub left_sum_hessian: f64,              // Left child hessian sum
    pub left_count: DataSize,               // Left child data count
    pub right_sum_gradient: f64,            // Right child gradient sum
    pub right_sum_hessian: f64,             // Right child hessian sum
    pub right_count: DataSize,              // Right child data count
    pub left_output: Score,                 // Left child leaf output
    pub right_output: Score,                // Right child leaf output
    pub default_left: bool,                 // Missing value direction
    
    // ‚ùå MISSING: Monotonic constraint support (Issue #102)
    // pub monotone_type: i8,                // Monotonic constraint type (-1: decreasing, 0: none, 1: increasing)
    
    // ‚ùå MISSING: Categorical feature support (Issue #103)  
    // pub cat_threshold: Vec<u32>,          // Categorical split thresholds (bitset representation)
}
```

### Complete SplitInfo Structure (Target Implementation)
```rust
// Target structure matching C++ LightGBM implementation
pub struct SplitInfo {
    pub feature: FeatureIndex,              // Feature index for the split
    pub threshold_bin: BinIndex,            // Bin threshold for the split
    pub threshold_value: f64,               // Actual threshold value
    pub gain: f64,                          // Split gain (improvement in loss function)
    pub left_sum_gradient: f64,             // Left child gradient sum
    pub left_sum_hessian: f64,              // Left child hessian sum
    pub left_count: DataSize,               // Left child data count
    pub right_sum_gradient: f64,            // Right child gradient sum
    pub right_sum_hessian: f64,             // Right child hessian sum
    pub right_count: DataSize,              // Right child data count
    pub left_output: Score,                 // Left child leaf output
    pub right_output: Score,                // Right child leaf output
    pub default_left: bool,                 // Missing value direction
    pub monotone_type: i8,                  // Monotonic constraint type (-1: decreasing, 0: none, 1: increasing)
    pub cat_threshold: Vec<u32>,            // Categorical split thresholds (bitset representation)
}
```

### TreeNode Structure
```rust
pub struct TreeNode {
    left_child: Option<NodeIndex>,          // Left child node index
    right_child: Option<NodeIndex>,         // Right child node index
    parent: Option<NodeIndex>,              // Parent node index
    split_feature: Option<FeatureIndex>,    // Split feature index
    split_threshold: Option<f64>,           // Split threshold value
    split_bin: Option<BinIndex>,            // Split bin index
    leaf_output: Option<Score>,             // Leaf prediction value
    sum_gradients: f64,                     // Sum of gradients in node
    sum_hessians: f64,                      // Sum of hessians in node
    data_count: DataSize,                   // Number of data points
    split_gain: f64,                        // Split gain value
    depth: usize,                           // Node depth in tree
    is_leaf: bool,                          // Whether node is a leaf
    default_left: bool,                     // Missing value default direction
}
```

### FeatureHistogram Structure
```rust
pub struct FeatureHistogram {
    data: Vec<Hist>,                        // Interleaved [grad, hess, grad, hess, ...]
    num_bins: usize,                        // Number of bins
    feature_index: FeatureIndex,            // Feature index
}
```

### Dataset Structure
```rust
pub struct Dataset {
    pub features: Array2<f32>,              // Feature matrix (num_data √ó num_features)
    pub num_data: DataSize,                 // Number of data points
    pub num_features: usize,                // Number of features
    pub bin_mappers: Vec<BinMapper>,        // Bin mappers for each feature
}
```

## Main Algorithm Flow

‚ö†Ô∏è **NOTE**: Current implementation uses breadth-first leaf processing (Issue #110). Should use max-gain leaf selection to match LightGBM C++ implementation.

```mermaid
flowchart TD
    Start([üü¢ Start: SerialTreeLearner::train]) --> ValidateInput[‚¨ú Validate dataset and gradient/hessian arrays]
    
    ValidateInput --> BeforeTrain[‚¨ú BeforeTrain(): Setup Data Structures]
    BeforeTrain --> InitTree[‚¨ú Initialize Tree with root capacity and shrinkage]
    InitTree --> CalcRootStats[‚¨° Calculate root statistics: total_gradients, total_hessians]
    CalcRootStats --> SetRootOutput[‚¨° Calculate and set root leaf output]
    
    SetRootOutput --> ForcedSplits{‚óá Forced Splits Defined?}
    ForcedSplits -->|Yes| ApplyForced[‚¨ú ForceSplits(): Apply JSON-defined splits]
    ForcedSplits -->|No| SplitLoop[‚¨ú Initialize split counter and leaf tracking]
    ApplyForced --> SplitLoop
    
    SplitLoop --> ClearCache[‚¨ú Clear histogram cache for reuse]
    
    ClearCache --> CheckLeaves{‚óá num_leaves < max_leaves?}
    CheckLeaves -->|No| FinalizeOutputs[‚¨ú Finalize leaf outputs for all leaf nodes]
    CheckLeaves -->|Yes| FindBestSplits[‚¨ú find_best_splits() for all candidate leaves]
    
    FindBestSplits --> SelectBest[‚¨ú Select leaf with maximum split gain]
    SelectBest --> ValidGain{‚óá Best gain > min_split_gain?}
    ValidGain -->|No| FinalizeOutputs
    ValidGain -->|Yes| CheckDepth{‚óá Selected leaf depth >= max_depth?}
    
    CheckDepth -->|Yes| FinalizeOutputs
    CheckDepth -->|No| ApplySplit[‚¨ú apply_split() - partition data and create children]
    
    ApplySplit --> CacheHistograms{‚óá Use histogram subtraction optimization?}
    CacheHistograms -->|Yes| CacheParent[‚¨ú Cache parent histograms for subtraction]
    CacheHistograms -->|No| UpdateLeafCount[‚¨ú Increment num_leaves]
    CacheParent --> UpdateLeafCount
    
    UpdateLeafCount --> SplitLoop
    
    FinalizeOutputs --> QuantizedCheck{‚óá Use Quantized Gradients?}
    QuantizedCheck -->|Yes| RenewLeaf[‚¨ú Renew leaf outputs with quantized gradients]
    QuantizedCheck -->|No| ReturnTree[üü¢ Return trained Tree]
    RenewLeaf --> ReturnTree
```

## BeforeTrain Process Detail

‚ö†Ô∏è **NOTE**: Current Rust implementation lacks this detailed initialization process (Issue #111)

```mermaid
flowchart TD
    BeforeStart([üü¢ Start: BeforeTrain]) --> ResetPool[‚¨ú Reset histogram pool]
    ResetPool --> SampleFeatures[‚¨ú Column sampler: Reset features by tree]
    SampleFeatures --> InitTrain[‚¨ú Initialize training data with selected features]
    InitTrain --> InitPartition[‚¨ú DataPartition::Init() - Set all data to root leaf]
    InitPartition --> ResetConstraints[‚¨ú Reset monotonic constraints]
    ResetConstraints --> ResetSplits[‚¨ú Reset best_split_per_leaf_ array]
    ResetSplits --> ComputeRoot[‚¨° Compute root leaf statistics]
    
    ComputeRoot --> UseBagging{‚óá Using bagging?}
    UseBagging -->|Yes| BaggingInit[‚¨ú smaller_leaf_splits_->Init(leaf=0, partition, gradients, hessians)]
    UseBagging -->|No| FullInit[‚¨ú smaller_leaf_splits_->Init(gradients, hessians)]
    
    BaggingInit --> InitLarger[‚¨ú larger_leaf_splits_->Init()]
    FullInit --> InitLarger
    InitLarger --> CEGBCheck{‚óá Cost-effective gradient boosting enabled?}
    CEGBCheck -->|Yes| CEGBInit[‚¨ú cegb_->BeforeTrain()]
    CEGBCheck -->|No| QuantBitsCheck{‚óá Quantized gradients enabled?}
    CEGBInit --> QuantBitsCheck
    QuantBitsCheck -->|Yes| SetHistBits[‚¨ú Set histogram bits for root leaf]
    QuantBitsCheck -->|No| BeforeEnd[üü¢ End: BeforeTrain]
    SetHistBits --> BeforeEnd
```

## Split Finding Algorithm Detail

‚ö†Ô∏è **NOTE**: Current implementation missing constraint validation and CEGB integration (Issues #102, #103, #108)

```mermaid
flowchart TD
    FindStart([üü¢ Start: find_best_split]) --> CheckMinData{‚óá Node has sufficient data?}
    CheckMinData -->|No| ReturnNone[üî¥ Return None - cannot split]
    CheckMinData -->|Yes| CheckMinHessian{‚óá Node has sufficient hessian sum?}
    CheckMinHessian -->|No| ReturnNone
    CheckMinHessian -->|Yes| SampleFeatures[‚¨ú FeatureSampler::sample_features()]
    
    SampleFeatures --> CheckSampled{‚óá Sampled features available?}
    CheckSampled -->|No| ReturnNone
    CheckSampled -->|Yes| FilterConstraints[‚¨ú ConstraintManager::filter_candidate_features()]
    
    FilterConstraints --> CheckFiltered{‚óá Allowed features available?}
    CheckFiltered -->|No| ReturnNone
    CheckFiltered -->|Yes| ConstructHistograms[‚¨ú construct_node_histograms() for allowed features]
    
    ConstructHistograms --> CacheForSubtraction{‚óá Use histogram subtraction?}
    CacheForSubtraction -->|Yes| CacheHists[‚¨ú Cache histograms for future subtraction]
    CacheForSubtraction -->|No| IterateFeatures[‚¨ú Initialize best_split = None]
    CacheHists --> IterateFeatures
    
    IterateFeatures --> FeatureLoop[‚¨ú For each allowed feature]
    FeatureLoop --> GetBinMapper[‚¨ú Get BinMapper for current feature]
    GetBinMapper --> CheckNumerical{‚óá Numerical feature?}
    
    CheckNumerical -->|Yes| CheckMonotone{‚óá Monotonic constraints enabled?}
    CheckNumerical -->|No| CategoricalSplit[‚¨° FindBestThresholdCategorical()]
    
    CheckMonotone -->|Yes| RecomputeConstraints[‚¨ú Recompute constraints for feature]
    CheckMonotone -->|No| NumericalSplit[‚¨° FindBestThreshold()]
    RecomputeConstraints --> NumericalSplit
    
    NumericalSplit --> CheckQuantized{‚óá Using quantized gradients?}
    CheckQuantized -->|Yes| QuantizedThreshold[‚¨° FindBestThresholdInt()]
    CheckQuantized -->|No| SetFeature[‚¨ú new_split.feature = real_feature_index]
    QuantizedThreshold --> SetFeature
    CategoricalSplit --> SetFeature
    
    SetFeature --> CEGBCheck{‚óá CEGB enabled?}
    CEGBCheck -->|Yes| ApplyCEGB[‚¨° new_split.gain -= cegb_->DeltaGain()]
    CEGBCheck -->|No| MonotoneCheck{‚óá Monotonic split?}
    ApplyCEGB --> MonotoneCheck
    
    MonotoneCheck -->|Yes| ApplyPenalty[‚¨° new_split.gain *= monotone_penalty]
    MonotoneCheck -->|No| ValidateConstraints[‚¨ú ConstraintManager::validate_split()]
    ApplyPenalty --> ValidateConstraints
    
    ValidateConstraints --> CheckSplitValid{‚óá Split valid and better than current best?}
    CheckSplitValid -->|Yes| UpdateBest[‚¨ú Update best_split = new split]
    CheckSplitValid -->|No| NextFeature[‚¨ú Continue to next feature]
    UpdateBest --> NextFeature
    
    NextFeature --> CheckMoreFeatures{‚óá More features to process?}
    CheckMoreFeatures -->|Yes| FeatureLoop
    CheckMoreFeatures -->|No| ReturnBest[üü¢ Return best_split]
```

## Histogram Construction Process

```mermaid
flowchart TD
    HistStart([üü¢ Start: construct_node_histograms]) --> CheckParallel{‚óá Use parallel construction?}
    CheckParallel -->|Yes| ParallelConstruct[‚¨ú Parallel iteration over features using rayon]
    CheckParallel -->|No| SequentialConstruct[‚¨ú Sequential iteration over features]
    
    ParallelConstruct --> FeatureHist[‚¨ú For each feature: construct_feature_histogram()]
    SequentialConstruct --> FeatureHist
    
    FeatureHist --> CheckSIMD{‚óá Use SIMD optimization AND sufficient data?}
    CheckSIMD -->|Yes| SIMDConstruct[‚¨ú construct_feature_histogram_simd() with chunking]
    CheckSIMD -->|No| ScalarConstruct[‚¨ú construct_feature_histogram_scalar()]
    
    SIMDConstruct --> InitLocalHist[‚¨ú Initialize local histogram per thread chunk]
    InitLocalHist --> SIMDLoop[‚¨ú For each data_index in chunk]
    SIMDLoop --> GetBin[‚¨° bin = BinMapper::value_to_bin(feature_value)]
    GetBin --> AccumulateLocal[‚¨° local_histogram[bin*2] += gradient, local_histogram[bin*2+1] += hessian]
    AccumulateLocal --> CheckMoreData{‚óá More data in chunk?}
    CheckMoreData -->|Yes| SIMDLoop
    CheckMoreData -->|No| AtomicMerge[‚¨ú Atomically merge local histogram into main histogram]
    AtomicMerge --> HistReady[‚¨ú Feature histogram ready]
    
    ScalarConstruct --> ScalarLoop[‚¨ú For each data_index in node]
    ScalarLoop --> GetBinScalar[‚¨° bin = BinMapper::value_to_bin(feature_value)]
    GetBinScalar --> AccumulateScalar[‚¨° histogram[bin*2] += gradient, histogram[bin*2+1] += hessian]
    AccumulateScalar --> CheckMoreScalar{‚óá More data points?}
    CheckMoreScalar -->|Yes| ScalarLoop
    CheckMoreScalar -->|No| HistReady
    
    HistReady --> CheckSubtraction{‚óá Use histogram subtraction optimization?}
    CheckSubtraction -->|Yes| TrySubtraction[‚¨ú Check if parent and sibling histograms cached]
    CheckSubtraction -->|No| ReturnHistograms[üü¢ Return constructed histograms]
    
    TrySubtraction --> SubtractionAvailable{‚óá Cached histograms available?}
    SubtractionAvailable -->|Yes| PerformSubtraction[‚¨° current = parent - sibling (element-wise)]
    SubtractionAvailable -->|No| ReturnHistograms
    PerformSubtraction --> ReturnHistograms
```

## Split Finding for Single Feature

```mermaid
flowchart TD
    SingleStart([üü¢ Start: find_best_split_for_feature]) --> CheckBins{‚óá num_bins >= 2?}
    CheckBins -->|No| NoSplit[üî¥ Return None - insufficient bins]
    CheckBins -->|Yes| InitBestSplit[‚¨ú Initialize SplitInfo with feature index]
    
    InitBestSplit --> InitAccumulators[‚¨ú left_sum_gradient = 0, left_sum_hessian = 0, left_count = 0]
    InitAccumulators --> BinLoop[‚¨ú For bin = 0 to num_bins-2]
    
    BinLoop --> AccumulateLeft[‚¨° left_sum_gradient += histogram[bin*2]]
    AccumulateLeft --> AccumulateLeftHess[‚¨° left_sum_hessian += histogram[bin*2+1]]
    AccumulateLeftHess --> AccumulateCount[‚¨° left_count += histogram[bin*2+1] (approximate)]
    
    AccumulateCount --> CalcRight[‚¨° right_sum_gradient = total_sum_gradient - left_sum_gradient]
    CalcRight --> CalcRightHess[‚¨° right_sum_hessian = total_sum_hessian - left_sum_hessian]
    CalcRightHess --> CalcRightCount[‚¨° right_count = total_count - left_count]
    
    CalcRightCount --> CheckConstraints{‚óá left_count >= min_data_in_leaf AND right_count >= min_data_in_leaf AND left_sum_hessian >= min_sum_hessian_in_leaf AND right_sum_hessian >= min_sum_hessian_in_leaf?}
    CheckConstraints -->|No| NextBin[‚¨ú Continue to next bin]
    CheckConstraints -->|Yes| CalcGain[‚¨° gain = calculate_split_gain()]
    
    CalcGain --> CheckGain{‚óá gain > best_split.gain AND gain > min_split_gain?}
    CheckGain -->|Yes| UpdateSplit[‚¨ú Update best_split with current split info]
    CheckGain -->|No| NextBin
    UpdateSplit --> SetDefaultDir[‚¨ú default_left = (left_sum_hessian >= right_sum_hessian)]
    SetDefaultDir --> NextBin
    
    NextBin --> CheckMoreBins{‚óá More bins to process?}
    CheckMoreBins -->|Yes| BinLoop
    CheckMoreBins -->|No| CheckValid{‚óá best_split.is_valid()?}
    
    CheckValid -->|Yes| CalcOutputs[‚¨° best_split.calculate_outputs(lambda_l1, lambda_l2)]
    CheckValid -->|No| NoValidSplit[üî¥ Return None - no valid split found]
    CalcOutputs --> ReturnSplit[üü¢ Return Some(best_split)]
```

## Mathematical Formulas

### 1. Leaf Output Calculation

The leaf output is calculated using Newton-Raphson optimization with L1/L2 regularization:

**Formula:**
```
leaf_output = -ThresholdL1(sum_gradients, Œª‚ÇÅ) / (sum_hessians + Œª‚ÇÇ)
```

**With L1 regularization (ThresholdL1):**
```rust
fn calculate_leaf_output(sum_gradient: f64, sum_hessian: f64, lambda_l1: f64, lambda_l2: f64) -> Score {
    if sum_hessian <= 0.0 {
        return 0.0;
    }

    let numerator = if lambda_l1 > 0.0 {
        if sum_gradient > lambda_l1 {
            sum_gradient - lambda_l1
        } else if sum_gradient < -lambda_l1 {
            sum_gradient + lambda_l1
        } else {
            0.0
        }
    } else {
        sum_gradient
    };

    (-numerator / (sum_hessian + lambda_l2)) as Score
}
```

**Path Smoothing (Issue #105 - Missing in current implementation):**
```rust
// After calculating base leaf output, apply path smoothing if enabled
if path_smooth > 0.0 {
    let smoothing_factor = (data_count as f64 / path_smooth) / (data_count as f64 / path_smooth + 1.0);
    let parent_factor = 1.0 / (data_count as f64 / path_smooth + 1.0);
    leaf_output = leaf_output * smoothing_factor + parent_output * parent_factor;
}
```

**Complete path smoothing formula from C++:**
```
smoothed_output = output √ó (n/Œ±) / (n/Œ± + 1) + parent_output / (n/Œ± + 1)
```

**Variables:**
- `sum_gradients` (f64): Sum of first-order gradients in node
- `sum_hessians` (f64): Sum of second-order gradients (Hessians) in node  
- `Œª‚ÇÅ` (f64): L1 regularization parameter (`lambda_l1`)
- `Œª‚ÇÇ` (f64): L2 regularization parameter (`lambda_l2`)
- `Œ±` (f64): Path smoothing parameter (`path_smooth`) - Missing in current implementation
- `n` (DataSize): Number of data points in leaf
- `parent_output` (f64): Output of parent node - Required for path smoothing

### 2. Split Gain Calculation

The gain from a split is computed using the exact mathematical formula matching C++ LightGBM implementation:

**Complete Mathematical Formula:**
```
gain = G_left¬≤/(H_left + Œª‚ÇÇ) + G_right¬≤/(H_right + Œª‚ÇÇ) - G_parent¬≤/(H_parent + Œª‚ÇÇ)
```

**With L1 regularization (exact C++ formula):**
```
gain = ThresholdL1(G_left, Œª‚ÇÅ)¬≤/(H_left + Œª‚ÇÇ) + ThresholdL1(G_right, Œª‚ÇÅ)¬≤/(H_right + Œª‚ÇÇ) - ThresholdL1(G_parent, Œª‚ÇÅ)¬≤/(H_parent + Œª‚ÇÇ)

where:
ThresholdL1(g, Œª‚ÇÅ) = sign(g) √ó max(0, |g| - Œª‚ÇÅ)
```

**Individual leaf gain calculation (bit-exact C++ implementation):**
```rust
fn calculate_leaf_gain_exact(sum_gradient: f64, sum_hessian: f64, lambda_l1: f64, lambda_l2: f64) -> f64 {
    if sum_hessian + lambda_l2 <= 0.0 {
        return 0.0;
    }

    let abs_sum_gradient = sum_gradient.abs();
    
    if abs_sum_gradient <= lambda_l1 {
        0.0
    } else {
        let numerator = if sum_gradient > 0.0 {
            sum_gradient - lambda_l1
        } else {
            sum_gradient + lambda_l1
        };
        (numerator * numerator) / (2.0 * (sum_hessian + lambda_l2))
    }
}
```

**Constraint validation (monotonic):**
```rust
// Monotonic constraint check (Issue #102 - Missing in current implementation)
if monotone_type > 0 && left_output > right_output { return 0.0; }
if monotone_type < 0 && left_output < right_output { return 0.0; }
```

**Variables:**
- `G_left`, `G_right`, `G_parent` (f64): Sum of gradients in left child, right child, and parent
- `H_left`, `H_right`, `H_parent` (f64): Sum of hessians in left child, right child, and parent
- `Œª‚ÇÅ` (f64): L1 regularization parameter (`lambda_l1`)
- `Œª‚ÇÇ` (f64): L2 regularization parameter (`lambda_l2`)
- `monotone_type` (i8): Constraint type (-1: decreasing, 0: none, 1: increasing)

### 3. Histogram Construction

Histograms use interleaved storage for cache efficiency:

**Storage format:**
```
histogram = [grad_bin0, hess_bin0, grad_bin1, hess_bin1, ..., grad_binN, hess_binN]
```

**Accumulation formula:**
```rust
// For each data point
let bin = bin_mapper.value_to_bin(feature_value);
histogram[bin * 2] += gradient;      // Gradient accumulation
histogram[bin * 2 + 1] += hessian;   // Hessian accumulation
```

**Optimized access:**
```rust
impl FeatureHistogram {
    #[inline]
    pub fn get_gradient(&self, bin: usize) -> Hist {
        self.data[bin << 1]  // Equivalent to data[bin * 2]
    }
    
    #[inline] 
    pub fn get_hessian(&self, bin: usize) -> Hist {
        self.data[(bin << 1) + 1]  // Equivalent to data[bin * 2 + 1]
    }
}
```

### 4. Histogram Subtraction Optimization

Core optimization from LightGBM for efficient sibling histogram construction:

**Formula:**
```
histogram_child = histogram_parent - histogram_sibling
```

**Implementation:**
```rust
pub fn subtract_from(&mut self, parent: &FeatureHistogram, sibling: &FeatureHistogram) {
    for i in 0..self.data.len() {
        self.data[i] = parent.data[i] - sibling.data[i];
    }
}
```

**Strategy:**
- Build histogram for smaller child directly
- Subtract from parent to get larger child histogram
- Reduces computation by ~50% for sibling nodes

### 5. Quantized Gradients (Issue #109 - Missing in current implementation)

When enabled, gradients and hessians are quantized to reduce memory usage:

**Quantization formula:**
```rust
// Quantization (missing in current implementation)
let int_gradient = (gradient / grad_scale).round() as i32;
let int_hessian = (hessian / hess_scale).round() as i32;
let packed_value = ((int_gradient as u64) << 32) | (int_hessian as u32 as u64);
```

**Dequantization formula:**
```rust
// Dequantization (missing in current implementation)  
let int_gradient = (packed_value >> 32) as i32;
let int_hessian = (packed_value & 0xFFFFFFFF) as i32;
let gradient = int_gradient as f64 * grad_scale;
let hessian = int_hessian as f64 * hess_scale;
```

**Variables:**
- `grad_scale`, `hess_scale` (f64): Scaling factors for quantization
- `packed_value` (u64): 64-bit packed gradient and hessian
- Benefit: Reduced memory footprint and improved cache efficiency

### 6. Feature Binning

Numerical features are discretized using quantile-based binning:

**Quantile binning:**
```rust
for i in 0..max_bins {
    let quantile = (i + 1) as f64 / max_bins as f64;
    let index = ((quantile * n as f64) as usize).min(n - 1);
    bin_upper_bounds.push(sorted_values[index] as f64);
}
```

**Bin mapping:**
```rust
pub fn value_to_bin(&self, value: f32) -> BinIndex {
    for (i, &upper_bound) in self.bin_upper_bounds.iter().enumerate() {
        if value as f64 <= upper_bound {
            return i as BinIndex;
        }
    }
    (self.num_bins - 1) as BinIndex
}
```

## Algorithm Components Detail

### Tree Construction Strategy

**Leaf-wise Growth:**
- Always process nodes in breadth-first order using `VecDeque<NodeInfo>`
- Split selection based on maximum gain across all candidate leaves
- Early stopping when `max_leaves` or `max_depth` reached

### Parallel Processing

**Feature-level Parallelization:**
```rust
// Parallel histogram construction across features
let histograms: Result<Vec<_>, _> = (0..num_features)
    .into_par_iter()
    .map(|feature_idx| {
        // Build histogram for each feature in parallel
    })
    .collect();
```

**SIMD Optimization:**
```rust
// Process data in chunks for better cache locality
data_indices
    .par_chunks(chunk_size)
    .try_for_each(|chunk| {
        // Process chunk with local histogram, then merge
    })
```

### Memory Management

**Histogram Pool:**
- Reuse histogram memory across nodes to reduce allocations
- LRU-style caching for constructed histograms
- Configurable pool size based on available memory

**Configuration:**
```rust
pub struct HistogramPoolConfig {
    pub max_bin: usize,           // Maximum bins per feature  
    pub num_features: usize,      // Number of features
    pub max_pool_size: usize,     // Maximum cached histograms
    pub initial_pool_size: usize, // Initial pool allocation
    pub use_double_precision: bool, // Use f64 vs f32
}
```

## Algorithmic References

### 1. Gradient Boosting Decision Trees (GBDT)
The core algorithm implements GBDT with Newton-Raphson optimization:
- **Reference**: "Greedy Function Approximation: A Gradient Boosting Machine" by Jerome H. Friedman
- **Newton-Raphson**: Uses second-order derivatives (Hessians) for optimal leaf values

### 2. Histogram-based Split Finding  
Efficient pre-sorted feature discretization approach:
- **Reference**: "LightGBM: A Highly Efficient Gradient Boosting Decision Tree" by Guolin Ke et al.
- **Key optimization**: Histogram subtraction trick reduces computation by 50%

### 3. Leaf-wise Tree Growth
Unlike level-wise growth, prioritizes leaves with highest loss reduction:
- **Strategy**: Split leaf with maximum gain first
- **Benefit**: Better loss reduction per leaf for complex patterns
- **Control**: `max_depth` prevents overfitting

### 4. Feature Sampling
Column sampling for regularization and speed:
- **Method**: Random sampling of features per split
- **Options**: `SamplingStrategy::Uniform` with configurable rates
- **Benefit**: Reduces overfitting and improves generalization

### 5. Regularization Techniques
L1/L2 regularization integrated into gain calculation:
- **L1 regularization**: Promotes sparsity via soft thresholding
- **L2 regularization**: Ridge-style penalty in denominator
- **Early stopping**: Minimum gain threshold prevents weak splits

## Performance Optimizations

### 1. Memory Layout Optimizations
```rust
// Interleaved storage for cache efficiency
struct FeatureHistogram {
    data: Vec<Hist>, // [grad0, hess0, grad1, hess1, ...]
}
```

### 2. Parallel Feature Processing
```rust
// Rayon-based parallelization
feature_indices
    .par_iter()
    .map(|&feature_idx| {
        // Parallel histogram construction
    })
```

### 3. SIMD-friendly Algorithms
```rust
// Chunked processing for vectorization
data_indices
    .par_chunks(chunk_size)
    .try_for_each(|chunk| {
        // Process in SIMD-friendly chunks
    })
```

### 4. Histogram Subtraction
```rust
// Avoid building both sibling histograms
fn subtract_from(&mut self, parent: &FeatureHistogram, sibling: &FeatureHistogram) {
    for i in 0..self.data.len() {
        self.data[i] = parent.data[i] - sibling.data[i];
    }
}
```

## Key Variables and Their Purposes

### Core Training Variables
| Variable | Type | Purpose |
|----------|------|---------|
| `features` | `Array2<f32>` | Feature matrix (num_data √ó num_features) |
| `gradients` | `ArrayView1<Score>` | First-order gradients for current iteration |
| `hessians` | `ArrayView1<Score>` | Second-order gradients (Hessians) |
| `bin_mappers` | `Vec<BinMapper>` | Feature discretization mappings |

### Tree Structure Variables  
| Variable | Type | Purpose |
|----------|------|---------|
| `nodes` | `Vec<TreeNode>` | Contiguous storage of all tree nodes |
| `num_leaves` | `usize` | Current number of leaf nodes |
| `max_leaves` | `usize` | Maximum allowed leaves |
| `shrinkage` | `f64` | Learning rate scaling factor |

### Node Information Variables
| Variable | Type | Purpose |
|----------|------|---------|
| `node_index` | `NodeIndex` | Node index in tree |
| `data_indices` | `Vec<DataSize>` | Data points belonging to node |
| `depth` | `usize` | Node depth in tree |
| `sum_gradients` | `f64` | Sum of gradients in node |
| `sum_hessians` | `f64` | Sum of hessians in node |
| `path_features` | `Vec<FeatureIndex>` | Features used in path to node |

### Configuration Variables
| Variable | Type | Purpose |
|----------|------|---------|
| `max_depth` | `usize` | Maximum tree depth allowed |
| `min_data_in_leaf` | `DataSize` | Minimum data points per leaf |
| `min_sum_hessian_in_leaf` | `f64` | Minimum hessian sum per leaf |
| `lambda_l1` | `f64` | L1 regularization parameter |
| `lambda_l2` | `f64` | L2 regularization parameter |
| `min_split_gain` | `f64` | Minimum gain threshold for splits |

This comprehensive flowchart captures the complete tree learning process in the Rust LightGBM implementation, from initialization through split finding to final tree construction, including all major mathematical formulations, data structures, and algorithmic optimizations that match the performance characteristics of the original C++ implementation.

## Missing Features

‚ö†Ô∏è **The current Rust implementation is missing several key features present in the C++ LightGBM implementation**:

### Critical Missing Features (High Priority)
| Feature | Status | GitHub Issue | Impact |
|---------|--------|--------------|--------|
| **Max-Gain Leaf Selection** | ‚ùå Missing | [#110](https://github.com/BectorVoom/pure-rust-lightgbm/issues/110) | Incorrect tree growth strategy |
| **Monotonic Constraints** | ‚ùå Missing | [#102](https://github.com/BectorVoom/pure-rust-lightgbm/issues/102) | No constraint validation |  
| **Categorical Features** | ‚ùå Missing | [#103](https://github.com/BectorVoom/pure-rust-lightgbm/issues/103) | Cannot handle categorical data |

### Advanced Optimization Features (Medium Priority)
| Feature | Status | GitHub Issue | Impact |
|---------|--------|--------------|--------|
| **Forced Splits (JSON)** | ‚ùå Missing | [#104](https://github.com/BectorVoom/pure-rust-lightgbm/issues/104) | No user-defined splits |
| **Cost-Effective Gradient Boosting** | ‚ùå Missing | [#108](https://github.com/BectorVoom/pure-rust-lightgbm/issues/108) | Reduced computational efficiency |
| **Quantized Gradients** | ‚ùå Missing | [#109](https://github.com/BectorVoom/pure-rust-lightgbm/issues/109) | Higher memory usage |
| **Path Smoothing** | ‚ùå Missing | [#105](https://github.com/BectorVoom/pure-rust-lightgbm/issues/105) | Missing regularization |
| **Feature Sampling** | ‚ùå Missing | [#106](https://github.com/BectorVoom/pure-rust-lightgbm/issues/106) | No column sampling |

### Documentation Issues (Low Priority)
| Issue | Status | GitHub Issue | Impact |
|-------|--------|--------------|--------|
| **Mathematical Formula Clarity** | ‚ö†Ô∏è Incomplete | [#107](https://github.com/BectorVoom/pure-rust-lightgbm/issues/107) | Less explicit documentation |

### Currently Implemented Features ‚úÖ
- Basic histogram construction with interleaved storage
- Newton-Raphson leaf output calculation with L1/L2 regularization
- Histogram subtraction optimization
- Parallel feature processing
- Basic split finding algorithm
- Tree structure management
- SIMD-optimized histogram construction

### Architectural Differences from C++ Implementation
1. **Language-Specific**: Uses Rust types (`DataSize`, `Score`, `Hist`) vs C++ types (`data_size_t`, `score_t`, `hist_t`)
2. **Memory Management**: Uses Rust ownership model instead of C++ smart pointers
3. **Parallelization**: Uses Rayon instead of OpenMP
4. **Data Structures**: Uses `ndarray` instead of custom C++ containers

For a complete comparison analysis, see `FLOWCHART_COMPARISON_ANALYSIS.md`.